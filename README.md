# ðŸ§ª Lab Assist - Voice-Powered Lab Data Entry

An intelligent voice assistant for laboratory animal data management with offline AI-powered natural language understanding.

## ðŸš€ Quick Start

```bash
# Complete setup with optimal AI models
make install

# Or use npm shortcuts
npm run setup        # Full setup
npm run setup-light  # Lightweight setup  
npm run setup-fast   # Ultra-fast llama.cpp setup

# Start the application
npm start
```

## âœ¨ Key Features

### ðŸŽ¤ **Voice Command Understanding**
- **Natural Language**: "weigh rat number five in cage three at two eighty grams"  
- **Flexible Phrasing**: "rat 5 cage 3 weight 280" or "move animal seven to cage twelve"
- **Word Numbers**: "two hundred eighty" â†’ 280, "five" â†’ 5
- **Context Aware**: "change weight to 300" remembers last rat

### ðŸ§  **Adaptive AI System**
- **4 Intelligence Levels**: Classification â†’ llama.cpp â†’ Ollama Light â†’ Ollama Full
- **Auto-Selection**: Picks fastest available backend automatically  
- **Smart Fallbacks**: Gracefully degrades if AI models fail
- **Performance Monitoring**: Switches backends based on speed/accuracy

### ðŸ“Š **Database Management**
- **SQLite Database**: Offline data storage with full ACID compliance
- **Session Tracking**: Context-aware commands across recording sessions
- **Data Integrity**: Automatic backups and transaction safety
- **Query System**: Find animals by weight ranges, cage locations

### âš¡ **Performance Optimized**
- **Ultra-Fast**: Classification mode <10ms, llama.cpp ~100ms
- **Lightweight**: Runs on laptops, no internet required
- **Memory Efficient**: 50MB-4GB depending on AI model choice
- **Offline First**: Complete functionality without network

## ðŸŽ¯ Supported Commands

### Data Recording
```
"rat 5 cage 3 weight 280 grams"
"weigh rat number five in cage three at two eighty"  
"animal seven cage twelve two hundred fifty grams"
```

### Weight Updates  
```
"change weight to 300 grams"
"update the weight to three hundred"
"set weight 275"
```

### Animal Movement
```
"move rat 7 to cage 12"
"transfer animal five into cage eight"
"relocate rat number three to cage fifteen"
```

### Data Queries
```
"show rats around 250 grams"
"find animals around two fifty" 
"what rats weigh around 300"
```

### System Control
```
"stop listening"
"start recording"
"pause session"
```

## ðŸ—ï¸ Architecture

```
Voice Input â†’ Whisper STT â†’ Adaptive NLU â†’ Command Service â†’ SQLite DB
                              â†“
                        [llama.cpp] â†’ [Ollama] â†’ [Classification]
                         (Fastest)   (Balanced)   (Fallback)
```

### AI Backend Options

| Backend | Speed | Memory | Accuracy | Use Case |
|---------|-------|--------|----------|----------|
| **Classification** | <10ms | <50MB | 85% | Ultra-fast fallback |
| **llama.cpp** | ~100ms | ~2GB | 95% | **Recommended balance** |
| **Ollama Light** | ~300ms | ~3GB | 95% | Good accuracy |
| **Ollama Full** | ~500ms | ~4GB | 97% | Maximum accuracy |

## ðŸ› ï¸ Setup Options

### Option 1: Automated (Recommended)
```bash
make install     # Downloads best available AI models
make status      # Check what's installed
make benchmark   # Test performance
```

### Option 2: Choose Your AI Level
```bash
# Ultra-fast inference
make setup-llama-cpp   # ~2GB, 100ms inference

# Balanced performance  
make setup-light-model # ~2GB, 300ms inference

# Maximum accuracy
make setup-models      # ~3GB, 500ms inference

# Minimal install (classification only)
npm install && npm run build
```

### Option 3: Manual Selection
```bash
# Check available options
make model-sizes

# Install specific model
make setup-tiny-model   # TinyLlama (~637MB)
make setup-light-model  # Phi-3 Mini (~2.3GB)
```

## ðŸ”§ Configuration

The app now defaults to **TinyLlama (637MB)** for the best balance of AI capability and small footprint. All configuration is centralized and easily customizable.

### Quick Configuration
```bash
# Check current configuration
npm run config

# List all available options with sizes
npm run config list

# Use default TinyLlama model (637 MB)
npm start

# Use ultra-lightweight mode (0 MB)
LAB_ASSIST_AI_BACKEND=classification npm start

# Use balanced model (2.3 GB)  
LAB_ASSIST_PRESET=balanced npm start

# Use performance mode (4+ GB)
LAB_ASSIST_PRESET=performance npm start
```

### Environment Variables
```bash
export LAB_ASSIST_AI_BACKEND=classification  # Force specific backend
export LAB_ASSIST_PRESET=tiny               # Use preset configuration
export MAX_INFERENCE_TIME=1000              # Timeout threshold  
```

### AI Backend Options (by size)
- **ollama-tiny** (637 MB): TinyLlama model (**default**) via `make quick-tiny`
- **classification** (0 MB): Pattern matching, ultra-fast, no download
- **ollama-light** (2.3 GB): Phi-3 Mini via `make quick-light`  
- **ollama-full** (4+ GB): Full models via `make install`
- **llamacpp** (Variable): GGUF models via `make setup-llama-cpp`

### Configuration Architecture
All settings are centralized in `src/shared/app-config.ts`:
- **Model Names**: `AI_MODELS.OLLAMA_TINY = 'tinyllama:1.1b'`
- **Timeouts**: `TIMEOUTS.AI_INFERENCE_DEFAULT = 1000`
- **Paths**: `PATHS.MODELS_DIR`, `PATHS.DATABASE_NAME`
- **Backend Info**: `BACKEND_CONFIG` with sizes, descriptions, setup commands
- **Presets**: `CONFIG_PRESETS` for different use cases

No more hardcoded values scattered across files!

### Runtime Backend Switching
```typescript
// Check current backend
const backend = commandService.getCurrentNLUBackend();
console.log(backend); // { backend: 'llamacpp', stats: { avgTime: 120, successRate: 0.98 }}

// Switch manually
await commandService.switchNLUBackend('classification'); // Fastest
await commandService.switchNLUBackend('llamacpp');       // Balanced  
await commandService.switchNLUBackend('ollama-light');   // Accurate
```

## ðŸ“Š Performance Monitoring

```bash
# Real-time status
npm run status

# Benchmark all models
npm run benchmark  

# Test AI understanding
npm run test-ai

# Test database operations  
npm run test-backend
```

### Performance Dashboard
```typescript
const status = await commandService.getNLUStatus();
console.log(status);
// {
//   activeBackend: 'llamacpp',
//   availableBackends: ['llamacpp', 'ollama-light', 'classification'],
//   performance: {
//     llamacpp: { avgTime: 120, successRate: 0.98 },
//     classification: { avgTime: 8, successRate: 0.85 }
//   },
//   recommendations: ['GPU acceleration available for 3x speedup']
// }
```

## ðŸ³ Docker Deployment

```bash
# Build container with AI models
make docker-build

# Run with GPU acceleration
docker run --gpus all -p 3000:3000 lab-assist

# CPU-only deployment
docker run -p 3000:3000 lab-assist
```

## ðŸ” Troubleshooting

### Installation Issues
```bash
make clean && make install   # Clean reinstall
make status                  # Check dependencies
```

### Performance Issues  
```bash
make benchmark              # Test all backends
export NLU_BACKEND=classification  # Force fastest mode
```

### AI Model Issues
```bash
# Check Ollama status
ollama list

# Re-download models
make setup-light-model

# Use classification fallback
export NLU_BACKEND=classification
```

## ðŸ“ˆ Performance Results

**Test Setup**: Intel i7, 16GB RAM, no GPU
**Test Command**: "rat five cage three weight two eighty grams"

| Metric | Classification | llama.cpp | Ollama Light | Ollama Full |
|--------|---------------|-----------|-------------|-------------|
| **Inference Time** | 8ms | 120ms | 280ms | 450ms |
| **Memory Usage** | 50MB | 2.2GB | 2.8GB | 3.2GB |
| **Accuracy** | 85% | 95% | 95% | 97% |
| **CPU Usage** | 5% | 25% | 40% | 60% |

## ðŸŽ¯ Use Cases

### Research Labs
- **Animal Studies**: Weight tracking, cage management, data logging
- **Batch Processing**: Voice commands while hands are busy
- **Data Integrity**: Automatic validation and error detection

### Production Environments  
- **Quality Control**: Fast data entry with voice verification
- **Inventory Management**: Real-time stock updates via voice
- **Process Monitoring**: Voice-driven measurement logging

### Educational Settings
- **Student Labs**: Simplified data entry interface
- **Training**: Voice-guided experimental procedures  
- **Assessment**: Automated data collection for grading

## ðŸ”— Links

- **Setup Guide**: [LEANER-AI-SETUP.md](./LEANER-AI-SETUP.md)
- **AI Enhancement Details**: [AI-ENHANCEMENT.md](./AI-ENHANCEMENT.md)
- **Makefile Commands**: [Makefile](./Makefile)

## ðŸ“„ License

MIT - See LICENSE file for details.

---

**Ready to transform your lab data entry with intelligent voice commands!** ðŸš€